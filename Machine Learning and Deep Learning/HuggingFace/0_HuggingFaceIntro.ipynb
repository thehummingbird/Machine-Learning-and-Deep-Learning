{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0_HuggingFaceIntro.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4vcHvew9VWR"
      },
      "source": [
        "**Intro to ðŸ¤— Hugging Face APIs for NLP and Vision**\n",
        "\n",
        "This notebook introduces Hugging Face APIs based on all the documentation  provided by Hugging Face at https://huggingface.co/course/chapter1\n",
        "\n",
        "This aims to introduce simple yet powerful APIs to get started with Hugging Face without deep DL expertise in Python\n",
        "\n",
        "**Jargon Alert!!!!**\n",
        "\n",
        "*Hugging Face Hub -* This is a repository of pre-trained models and datasets which can be imported directly by anyone with just one line of code\n",
        "\n",
        "\n",
        "*Note - This notebook only deals with data preprocessing, inference, and postprocessing. Training/fine-tuning will be shown in the next notebook*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXf1ixuO-vbK",
        "outputId": "b9732bbd-fbcb-4cf0-fa59-2c2efe5d6d7a"
      },
      "source": [
        "'''\n",
        "First, let's install transformers\n",
        "'''\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMby7D1Z-FeO"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Experiment 1: Pipeline**\n",
        "\n",
        "Pipeline is an API to do inference using a pre-trained DL model (we refer to NLP models here) available in Hugging Face Hub in just 3 lines of code.\n",
        "\n",
        "It acutally does 3 different processes under the hood - preprocessing input data, passing it to the model, post processing model output to get an intelligible answer\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YzS-xDR9vy7",
        "outputId": "c86e83ad-6236-44b4-cc29-6ebcb885f26b"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "sentences = [\n",
        "             \"I think you can do better\",\n",
        "             \"This is a great camera from Amazon\"\n",
        "]\n",
        "results = classifier(sentences)\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'NEGATIVE', 'score': 0.9979079961776733}, {'label': 'POSITIVE', 'score': 0.9997420907020569}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFKU-e0GA7vr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "You saw that pipelines handled almost everything. All it needed was the **type** of pipeline (list of pipelines in [this](https://huggingface.co/models) lesson) and sentences as the input.\n",
        "\n",
        "We chose sentiment analysis here. But depending on the pipeline you choose, the results will be different.\n",
        "\n",
        "There are multiple models availabe in Hugging Face Hub to choose from for a task. Since we did not choose one, pipeline used the default model for sentiment analysis from hub (distilbert-base-uncased-finetuned-sst-2-english).\n",
        "\n",
        "We can always give a model name as well. To do that, go to [hub](https://huggingface.co/models) and choose the one you like. \n",
        "\n",
        "I like to choose based on the **Task** on the left side. You can also test the model by selecting it and playing with **Hosted Inference API**\n",
        "\n",
        "Here, I selected **siebert/sentiment-roberta-large-english**, which was one of the models under *Text Classification* task\n",
        "\n",
        "GO PLAY!\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7v5ucpVABYW",
        "outputId": "47e1eec9-d9fd-475f-8a72-044af40f4620"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
        "sentences = [\n",
        "             \"I think you can do better\",\n",
        "             \"This is a great camera from Amazon\"\n",
        "]\n",
        "results = classifier(sentences)\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'NEGATIVE', 'score': 0.9968762397766113}, {'label': 'POSITIVE', 'score': 0.9987062215805054}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctC184_FD_9z"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here's another example of pipeline for NLP tasks- question-answering\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr_N7uG6CVNV",
        "outputId": "38b234c7-5cf2-4e46-883d-f678ac08f61e"
      },
      "source": [
        "ques_ans = pipeline(\"question-answering\")\n",
        "context = \"Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tunea model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\"\n",
        "result = ques_ans(question=\"What is extractive question answering?\", context=context)\n",
        "print(result)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.616338312625885, 'start': 33, 'end': 94, 'answer': 'the task of extracting an answer from a text given a question'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GibXwkPqEqZN"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Experiment 2: Tokenizers and models**\n",
        "\n",
        "Remember we said pipeline does 3 things - preprocessing input, passing it to the model, and post processing? We now seperate these out to gain more control over the process.\n",
        "\n",
        "**Jargon Alert!!**\n",
        "\n",
        "Tokenizer - This takes care of step 1 and 3 (preprocessing and postprocessing data). It handles the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed\n",
        "\n",
        "Generally, tokenization is the process of dividing a sentence into tokens based on some set of rules in NLP. And that's what tokenizer does in one of the steps (More info on tokenizers [here](https://huggingface.co/course/chapter2/4?fw=pt))\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6RngOqzER-v",
        "outputId": "80777f71-421e-4584-f710-61be0760bfbb"
      },
      "source": [
        "#AutoTokenizer automatically selects the correct tokenization process based on the model selected (checkpoint)\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "'''\n",
        "This tokenizer will handle transforming raw input to the correct format\n",
        "which the chosen model understands. We return pytorch tensor here because\n",
        "the model needs tensors\n",
        "'''\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "raw_inputs = [\n",
        "    \" I am delighted with the product\",\n",
        "    \"This might not be the best idea\"\n",
        "]\n",
        "\n",
        "#proprocessed inputs\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  2572, 15936,  2007,  1996,  4031,   102,     0],\n",
            "        [  101,  2023,  2453,  2025,  2022,  1996,  2190,  2801,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkL5zMK6GjDf"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Tokenizer has converted text into 'input_ids', which will be used by the model!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "So let's talk about models now.\n",
        "There are two kinds of models - with heads and headless. This is based on the requirement of either feature extractions or some specific task like classification\n",
        "\n",
        "*Models without heads -* The last layer of the model is stripped and we get the results (feature vector) from the second last layer\n",
        "\n",
        "*Models with heads -* The last layer called the head decides the task to be done (like classification)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5E6IXTIGdGy",
        "outputId": "32177099-bf35-450d-a772-b89542c68590"
      },
      "source": [
        "#Model without Head. AutoModel automatically selects the correct model based on the model selected (checkpoint)\n",
        "from transformers import AutoModel\n",
        "'''\n",
        "This architecture only has base Transformer: given some input(from the previous step), output hidden states (features).\n",
        "It is an n-dimensional feature vector.\n",
        "'''\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "\n",
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 9, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BykEdRrJwxD"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "SO! We see a **2x9x768** dimensional tensor here.\n",
        "\n",
        "2 - number of inputs\n",
        "\n",
        "9 - number of tokens in each input\n",
        "\n",
        "768 - number of features for each input\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Now let's look at models with heads. \n",
        "\n",
        "We can choose a model using AutoModelFor*** to do this, which will select the appropriate head.\n",
        "\n",
        "In this examples, we classify prevously obtained inputs (from tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT03IF1pG7Jc",
        "outputId": "21fd2e3a-bd47-4fe5-98b8-c6c4b04a3c95"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "outputs = model(**inputs)\n",
        "\n",
        "#use softmax to getprobabilities\n",
        "import torch\n",
        "#softmax probablities\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "print(predictions)\n",
        "\n",
        "#classification results for both inputs\n",
        "results = torch.argmax(predictions, axis=1)\n",
        "print(results)\n",
        "#classification labels mapping to results\n",
        "print(model.config.id2label)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.1872e-04, 9.9988e-01],\n",
            "        [9.9974e-01, 2.5673e-04]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([1, 0])\n",
            "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cLws9J1hQOm"
      },
      "source": [
        "[link text](https://)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "While tokenizer + model together do the job of preprocessing -> inference -> postprocessing well, we go deeper into tokenization to understand them better.\n",
        "\n",
        "Tokenizers essentially perform encoding (tokenization and getting input ids from tokens)\n",
        "\n",
        "Translating text to numbers is known as encoding. \n",
        "\n",
        "Encoding is done in a two-step process: the tokenization, \n",
        "followed by the conversion to input IDs. \n",
        "\n",
        "*Note - Atomic operations a tokenizer can handle: tokenization, conversion to IDs, and converting IDs back to a string*\n",
        "\n",
        "We now demostrate this two step process below\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XruPW_NcM_fT",
        "outputId": "5e085a1f-6faa-4cf8-e6e0-9733c5328023"
      },
      "source": [
        "#Dividing tokenization into two steps\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "tokens = tokenizer.tokenize(\"Using a Transformer network is simple\")\n",
        "print(tokens)\n",
        "\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n",
            "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51bdHFLijGZI"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "And if we convert these IDs back to a string (decode), we get the text input back below\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqwfz5L7jBLp",
        "outputId": "33e6b947-17c5-4186-d174-af4ff3f12835"
      },
      "source": [
        "decoded_string = tokenizer.decode(ids)\n",
        "print(decoded_string)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using a Transformer network is simple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuVG8bvSiZl2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Now comparing it with the two step process automatically done by the tokenizer, we get the same result for input ids below (101 and 102 just mark the start and end of sentences)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4EVWOLBiO-U",
        "outputId": "650e1fdc-7f09-4dc4-e96d-fb167a640ad7"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "tokens = tokenizer(\"Using a Transformer networks is simple\")  #use this always\n",
        "print(tokens)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 7993, 170, 13809, 23763, 6379, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkN8Yb2siyA2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Moving forward, we will always use this API to handle the two steps of tokenization automatically\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Now that we understand pipelines, tokenizers (and how they work under the hood) and models, we shall wrap it up by showing the preferred flow for inference below\n",
        "\n",
        "**The cell below is a summary of this notebook**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stJ0fMFaiV6b",
        "outputId": "85299150-4cea-4bf9-9a4a-afe6da89d56b"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\n",
        "    \"This is such a nice view\",\n",
        "    \"I think this is not up to the mark\"\n",
        "]\n",
        "\n",
        "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors='pt')\n",
        "outputs = model(**tokens)\n",
        "\n",
        "#use softmax to getprobabilities\n",
        "import torch\n",
        "#softmax probablities\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "print(predictions)\n",
        "\n",
        "#classification results for both inputs\n",
        "results = torch.argmax(predictions, axis=1)\n",
        "print(results)\n",
        "#classification labels mapping to results\n",
        "print(model.config.id2label)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.5476e-04, 9.9985e-01],\n",
            "        [9.9977e-01, 2.3102e-04]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([1, 0])\n",
            "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnMraQY1k04p"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Bonus Tip**\n",
        "\n",
        "Remeber how we used pipeline in the beginning of this notebook?\n",
        "We can do that too.\n",
        "\n",
        "But pipelines do not give us a lot of control. For instance, we cannot fine-tune a model and use its weights, since it only allows us to choose a model from Hugging Face hub. So unless we have our weights on Hugging Face hub, we need to use the previous methods for that.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ-hDTNzkEry",
        "outputId": "c997a52b-230d-449b-b9c0-280c7fa87e53"
      },
      "source": [
        "from transformers import pipeline\n",
        "#also can use a pipleline: We started with this\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "sequences = [\n",
        "    \"This is such a nice view\",\n",
        "    \"I think this is not up to the mark\"\n",
        "]\n",
        "result = classifier(sequences)\n",
        "print(result)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9998452663421631}, {'label': 'NEGATIVE', 'score': 0.9997690320014954}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pboHv5LTlEXE"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}